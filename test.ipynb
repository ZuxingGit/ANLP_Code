{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text=\"My friend called me when my phone was charging I charge my phone everyday\"\n",
    "text = text.lower()\n",
    "print(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import PorterStemmer\n",
    "\n",
    "stemmer = PorterStemmer()\n",
    "\n",
    "# Define a sample text\n",
    "tokens = text.split()\n",
    "\n",
    "stemmed_text = ' '.join([stemmer.stem(token) for token in tokens])\n",
    "print(\"Original Text:\", text)\n",
    "print(\"Stemmed Text:\", stemmed_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import nltk\n",
    "# nltk.download('vader_lexicon')\n",
    "\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer \n",
    "from nltk.tokenize import word_tokenize\n",
    "# analyse with VADER\n",
    "analyser = SentimentIntensityAnalyzer()\n",
    "text_data=\"my friend called me when my phone was charging sadness fantastic enemy\"\n",
    "text_data = word_tokenize(text_data)\n",
    "for text in text_data:\n",
    "    score = analyser.polarity_scores(text) \n",
    "    if score['compound'] >= 0.05:\n",
    "        print(text+\": \"+\"VADER positive\") \n",
    "    elif score['compound'] <= -0.05:\n",
    "        print(text+\": \"+\"VADER negative\") \n",
    "    else:\n",
    "        print(text+\": \"+\"VADER neutral\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import spacy \n",
    "import requests \n",
    "from bs4 import BeautifulSoup\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "pd.set_option(\"display.max_rows\", 200)\n",
    "\n",
    "content = \"living room, king, 11/11/2011, Mr Bean, 20%, equator\"\n",
    " \n",
    "doc = nlp(content)\n",
    " \n",
    "for ent in doc.ents:\n",
    "    print(ent.text, ent.label_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "from spacy import displacy\n",
    "\n",
    "NER = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "raw_text=[\"living room\", \"king\", \"11/11/2011\", \"Mr Bean\", \"20%\", \"equator\"]\n",
    "# text1 = NER(raw_text)\n",
    "\n",
    "for element in raw_text:\n",
    "    for word in NER(element).ents:\n",
    "        print(word, word.label_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "\n",
    "nlp=spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "text = \"living room; king; 11/11/2011; Mr Bean; 20%; equator;\"\n",
    "\n",
    "doc = nlp(text)\n",
    "for ent in doc.ents:\n",
    "    print(ent, ent.label_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "text = \"Some text with a URL: https://example.com and another URL: http://another.com\"\n",
    "\n",
    "text = re.sub(r\"http\\S+\", \"\", text)\n",
    "print(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "text=['Hi', 'Hello', 'Ciao', 'Hi', 'Hello', 'Hi']\n",
    "counts = dict(Counter(text).most_common(2))\n",
    "\n",
    "print(counts)\n",
    "# for k, v in counts.items():\n",
    "#     print(k) \n",
    "#     print(v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "import spacy\n",
    "\n",
    "text = \"I would not stay here again, was would\"\n",
    "\n",
    "tokens = word_tokenize(text)\n",
    "print(tokens)\n",
    "\n",
    "stemmer = PorterStemmer()\n",
    "stemmed_text = [stemmer.stem(token) for token in tokens ]\n",
    "print(stemmed_text)\n",
    "\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "doc = nlp(text)\n",
    "lemmatized_tokens = [token.lemma_ for token in doc]\n",
    "print(lemmatized_tokens)\n",
    "\n",
    "stop_words = set(stopwords.words('english'))\n",
    "filtered_words = [word for word in tokens if word.lower() not in stop_words]\n",
    "filtered_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.sentiment import SentimentIntensityAnalyzer\n",
    "\n",
    "sia = SentimentIntensityAnalyzer()\n",
    "text = \"Rude staff, extra charges, overall unimpressive We traveled here with a big group for a wedding the weekend of the 16th. Immediately upon check-in the man at the front desk was lazy, uninterested and every little question regarding the reservation seemed to bother him. The woman was SO rude. She looked at us like we were scum, it was horrible. We had been booked into a non-smoking room and after some coaxing we got him to move us. Joey was nice but later when we had problems with overcharging it was like a completely different person. Definitely not in a good way. \\nGood: The room was clean and the bar staff was SO much fun. We spent a lot of time in there and the manager was so great. He remembered us and joked with us, that was a truly nice experience. Buzz the bartender is so sweet also. I left my card there (never closed my tab! Yikes!) and they had no problem mailing it to me ALL the way in Colorado). I even saw the woman from Front Desk go into the bar after her shift for a few drinks and she seemed very capable of kindness then but spared none for us guests.\\nThe hotel is nice overrall but seemed a little sad. Really enjoyed the 8ft deep pool and hot tub. When we got home however, they had charged us for 2 rooms even though we only stayed in one and there were 6 other mysterious 100+ dollar charges which the Front Desk people still haven't been figured out. If it wasn't for the bar and it's staff, I would rate this place as Terrible\"\n",
    "sia.polarity_scores(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"not better\"\n",
    "sia.polarity_scores(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import sent_tokenize\n",
    "\n",
    "# Download required resources\n",
    "nltk.download('punkt')\n",
    "\n",
    "# Sample article\n",
    "article = \"\"\"\n",
    "Dr. Smith went to Washington. He met with Mr. Johnson, who works at Google Inc.\n",
    "They discussed the future of AI. In the evening, Dr. Smith attended a dinner with several AI experts.\n",
    "This isn?t the same style of Republican majority pushed from power after being routed in the 2006 midterm elections \n",
    "after the public backlash to the administration of President George W. Bush and his handling of the war in Iraq.\n",
    "\"\"\"\n",
    "\n",
    "# Split the article into sentences\n",
    "sentences = sent_tokenize(article)\n",
    "\n",
    "# Print the split sentences\n",
    "for i, sentence in enumerate(sentences):\n",
    "    print(f\"Sentence {i + 1}: {sentence}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install transformers sentence-transformers\n",
    "from sentence_transformers import SentenceTransformer, util\n",
    "\n",
    "# Load pre-trained Sentence-BERT model\n",
    "model = SentenceTransformer('bert-base-nli-mean-tokens')\n",
    "\n",
    "# Sample query and article\n",
    "query = \"What is the role of the CLS token in BERT?\"\n",
    "article = \"\"\"\n",
    "BERT, which stands for Bidirectional Encoder Representations from Transformers, is a pre-trained language model developed by Google. It is designed to understand the context of a word in search queries, thereby improving the relevance of search results. The [CLS] token is a special token added to the beginning of the input sequence and is used to aggregate information from the entire sequence for classification tasks.\n",
    "\"\"\"\n",
    "\n",
    "# Encode the query and the article\n",
    "query_embedding = model.encode(query, convert_to_tensor=True)\n",
    "article_embedding = model.encode(article, convert_to_tensor=True)\n",
    "\n",
    "# Compute cosine similarity\n",
    "cosine_sim = util.pytorch_cos_sim(query_embedding, article_embedding)\n",
    "\n",
    "print(f\"Cosine Similarity: {cosine_sim.item()}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "%pip install datasets\n",
    "from datasets import load_metric\n",
    "import numpy as np\n",
    "from collections import Counter\n",
    "\n",
    "# Load a pre-trained QA model and tokenizer\n",
    "qa_pipeline = pipeline(\"question-answering\", model=\"distilbert-base-cased-distilled-squad\")\n",
    "\n",
    "# Sample question and context\n",
    "question = \"What is the role of the CLS token in BERT?\"\n",
    "context = \"\"\"\n",
    "BERT, which stands for Bidirectional Encoder Representations from Transformers, is a pre-trained language model developed by Google. It is designed to understand the context of a word in search queries, thereby improving the relevance of search results. The [CLS] token is a special token added to the beginning of the input sequence and is used to aggregate information from the entire sequence for classification tasks.\n",
    "\"\"\"\n",
    "\n",
    "# Predict the answer\n",
    "result = qa_pipeline(question=question, context=context)\n",
    "\n",
    "# Ground truth answers\n",
    "expected_answer = [\"Aggregate information from the entire sequence for classification tasks\"]\n",
    "\n",
    "# Function to compute exact match\n",
    "def exact_match(prediction, expected_answer):\n",
    "    return int(any(prediction == gt for gt in expected_answer))\n",
    "\n",
    "# Function to compute F1 score\n",
    "def f1_score(prediction, expected_answer):\n",
    "    def compute_f1(a_gold, a_pred):\n",
    "        gold_tokens = a_gold.split()\n",
    "        pred_tokens = a_pred.split()\n",
    "        common = Counter(gold_tokens) & Counter(pred_tokens)\n",
    "        num_same = sum(common.values())\n",
    "        if num_same == 0:\n",
    "            return 0\n",
    "        precision = 1.0 * num_same / len(pred_tokens)\n",
    "        recall = 1.0 * num_same / len(gold_tokens)\n",
    "        f1 = (2 * precision * recall) / (precision + recall)\n",
    "        return f1\n",
    "\n",
    "    return max(compute_f1(gt, prediction) for gt in expected_answer)\n",
    "\n",
    "# Evaluate the predictions\n",
    "print(\"Predicted Answer:\", result['answer'])\n",
    "em_score = exact_match(result['answer'], expected_answer)\n",
    "f1 = f1_score(result['answer'], expected_answer)\n",
    "\n",
    "print(f\"Exact Match: {em_score}\")\n",
    "print(f\"F1 Score: {f1}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertForQuestionAnswering, BertTokenizer\n",
    "import torch\n",
    "\n",
    "# Load pre-trained model and tokenizer\n",
    "model_name = \"bert-large-uncased-whole-word-masking-finetuned-squad\"\n",
    "model = BertForQuestionAnswering.from_pretrained(model_name)\n",
    "tokenizer = BertTokenizer.from_pretrained(model_name)\n",
    "\n",
    "# Define the question and context\n",
    "question = \"What is the role of the CLS token in BERT?\"\n",
    "context = \"\"\"\n",
    "BERT, which stands for Bidirectional Encoder Representations from Transformers, is a pre-trained language model developed by Google. It is designed to understand the context of a word in search queries, thereby improving the relevance of search results. The [CLS] token is a special token added to the beginning of the input sequence and is used to aggregate information from the entire sequence for classification tasks.\n",
    "\"\"\"\n",
    "\n",
    "# Tokenize input\n",
    "inputs = tokenizer.encode_plus(question, context, return_tensors='pt')\n",
    "\n",
    "# Get input IDs and attention mask\n",
    "input_ids = inputs['input_ids']\n",
    "attention_mask = inputs['attention_mask']\n",
    "\n",
    "# Get the scores for start and end of the answer\n",
    "outputs = model(input_ids, attention_mask=attention_mask)\n",
    "start_scores = outputs.start_logits\n",
    "end_scores = outputs.end_logits\n",
    "\n",
    "# Get the most likely beginning and end of the answer\n",
    "start_index = torch.argmax(start_scores)\n",
    "end_index = torch.argmax(end_scores) + 1\n",
    "\n",
    "# Convert tokens to string\n",
    "answer_tokens = tokenizer.convert_ids_to_tokens(input_ids[0][start_index:end_index])\n",
    "answer = tokenizer.convert_tokens_to_string(answer_tokens)\n",
    "\n",
    "# Get confidence scores\n",
    "start_score = torch.max(start_scores).item()\n",
    "end_score = torch.max(end_scores).item()\n",
    "\n",
    "# Average confidence score\n",
    "confidence_score = (start_score + end_score) / 2\n",
    "\n",
    "print(f\"Question: {question}\")\n",
    "print(f\"Answer: {answer}\")\n",
    "print(f\"Confidence Score: {confidence_score}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %pip install spacy==2.3.7\n",
    "\n",
    "import spacy\n",
    "print(spacy.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install cython==0.29\n",
    "%pip install git+https://github.com/huggingface/neuralcoref.git\n",
    "%pip install spacy==2.1.0 \n",
    "%pip install neuralcoref --no-binary neuralcoref\n",
    "%python -m spacy download en_core_web_sm\n",
    "\n",
    "# python 3.8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Load your usual SpaCy model (one of SpaCy English models)\n",
    "import spacy\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "\n",
    "# load NeuralCoref and add it to the pipe of SpaCy's model\n",
    "import neuralcoref\n",
    "coref = neuralcoref.NeuralCoref(nlp.vocab)\n",
    "nlp.add_pipe(coref, name='neuralcoref')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'My sister has a dog. My sister loves a dog. My sister named a dog Billy.'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Solve coreferences in sentences\n",
    "doc = nlp('My sister has a dog. She loves him. She named him Billy.')\n",
    "\n",
    "doc._.has_coref\n",
    "# True\n",
    "\n",
    "doc._.coref_clusters\n",
    "# [My sister: [My sister, She, She], a dog: [a dog, him, him]]\n",
    "\n",
    "doc._.coref_resolved\n",
    "# My sister has a dog. My sister loves a dog. My sister named a dog Billy."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
